{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary library\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to call the images and resizing them to 224 by 224 images, and store them in array lists\n",
    "def load_images(directory):\n",
    "    image_array = []\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".png\"):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            \n",
    "            # Load and resize the image\n",
    "            img = cv2.imread(filepath)\n",
    "            img = cv2.resize(img, (224, 224))\n",
    "            \n",
    "            image_array.append(img)\n",
    "    \n",
    "    return image_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Directories and start resizing, and list them into array lists\n",
    "normal_dir= 'D:/Latihan_Python/Datasets/Split_Data/Normal'  # Normal CXR Image Dataset\n",
    "viral_dir= 'D:/Latihan_Python/Datasets/Split_Data/Viral Pneumonia'    # Viral Pneumonia CXR Image Dataset\n",
    "covid_dir= 'D:/Latihan_Python/Datasets/Split_Data/COVID'    # COVID-19 CXR Image Dataset\n",
    "\n",
    "normal_dataset = load_images(normal_dir)\n",
    "viral_dataset = load_images(viral_dir)\n",
    "covid_dataset = load_images(covid_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Dataset which is made up of 2952 Image Arrays has: \n",
      "988 Normal CXR Image Arrays\n",
      "989 Viral Pneumonia CXR Image Arrays\n",
      "975 COVID-19 CXR Image Arrays\n"
     ]
    }
   ],
   "source": [
    "#Checking the array list\n",
    "print(\"The Dataset which is made up of {} Image Arrays has: \".format(len(normal_dataset) + len(viral_dataset) + len(covid_dataset)))\n",
    "print('{} Normal CXR Image Arrays'.format(len(normal_dataset)))\n",
    "print('{} Viral Pneumonia CXR Image Arrays'.format(len(viral_dataset)))\n",
    "print('{} COVID-19 CXR Image Arrays'.format(len(covid_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funtion to normalize the data, and save them in array lists\n",
    "def normalizer(image_arrays):\n",
    "\n",
    "\n",
    "    norm_image_arrays = []\n",
    "    \n",
    "    # Iterate over all the image arrays and normalize them before storing them into our predefined list\n",
    "    for image_array in image_arrays:\n",
    "        norm_image_array = image_array / 255.0\n",
    "        norm_image_arrays.append(norm_image_array)\n",
    "    \n",
    "    return norm_image_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing them in variable and do the normalization\n",
    "normal_dataset_normalized = normalizer(normal_dataset)\n",
    "viral_dataset_normalized = normalizer(viral_dataset)\n",
    "covid_dataset_normalized = normalizer(covid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def split_and_merge_function(image_arrays, split_factor=[0.7, 0.15, 0.15]):\n",
    "    datasets = {}\n",
    "    train_data, validation_data, test_data = [], [], []\n",
    "    train_labels, validation_labels, test_labels = [], [], []\n",
    "    \n",
    "    for image_array_id, image_array in enumerate(image_arrays):\n",
    "        labels = [image_array_id] * len(image_array)\n",
    "        \n",
    "        # Split the data and labels into train, validation, and test sets\n",
    "        train_data_temp, test_data_temp, train_labels_temp, test_labels_temp = train_test_split(image_array, labels, train_size=split_factor[0], random_state=42)\n",
    "        validation_data_temp, test_data_temp, validation_labels_temp, test_labels_temp = train_test_split(test_data_temp, test_labels_temp, train_size=split_factor[1] / (split_factor[1] + split_factor[2]), random_state=42)\n",
    "        \n",
    "        # Merge the data and labels into the respective lists\n",
    "        train_data.extend(train_data_temp)\n",
    "        train_labels.extend(train_labels_temp)\n",
    "        validation_data.extend(validation_data_temp)\n",
    "        validation_labels.extend(validation_labels_temp)\n",
    "        test_data.extend(test_data_temp)\n",
    "        test_labels.extend(test_labels_temp)\n",
    "        \n",
    "    # Store the train, validation, and test datasets into the datasets dictionary\n",
    "    datasets['train_dataset'] = np.array(train_data)\n",
    "    datasets['validation_dataset'] = np.array(validation_data)\n",
    "    datasets['test_dataset'] = np.array(test_data)\n",
    "    # Convert labels from label-encoding to one-hot encoding and store in the datasets dictionary\n",
    "    datasets['train_labels'] = to_categorical(np.array(train_labels))\n",
    "    datasets['validation_labels'] = to_categorical(np.array(validation_labels))\n",
    "    datasets['test_labels'] = to_categorical(np.array(test_labels))\n",
    "    return datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data set in to desired percentage\n",
    "image_arrays = [normal_dataset_normalized, viral_dataset_normalized, covid_dataset_normalized]\n",
    "datasets = split_and_merge_function(image_arrays, split_factor = [0.7, 0.15, 0.15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store it in variables\n",
    "train_dataset = datasets['train_dataset']\n",
    "validation_dataset = datasets['validation_dataset']\n",
    "test_dataset = datasets['test_dataset']\n",
    "train_labels = datasets['train_labels'] \n",
    "validation_labels = datasets['validation_labels']\n",
    "test_labels = datasets['test_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Dataset which is made up of 2952 Image Arrays has been splitted into:\n",
      "2065 Training Image Arrays\n",
      "442 Validation Image Arrays\n",
      "445 Test Image Arrays\n"
     ]
    }
   ],
   "source": [
    "#Checking the splitted dataset\n",
    "print(\"The Dataset which is made up of {} Image Arrays has been splitted into:\".format(len(train_dataset) + len(validation_dataset) + len(test_dataset)))\n",
    "print('{} Training Image Arrays'.format(len(train_dataset)))\n",
    "print('{} Validation Image Arrays'.format(len(validation_dataset)))\n",
    "print('{} Test Image Arrays'.format(len(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Splitted Dataset\n",
    "\n",
    "# Create a directory to save the split datasets\n",
    "save_dir = 'D:/Latihan_Python/Notebooks/Tugas Akhir/Splited_dataset'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Save the train, validation, and test datasets\n",
    "np.save(os.path.join(save_dir, 'train_dataset.npy'), train_dataset)\n",
    "np.save(os.path.join(save_dir, 'validation_dataset.npy'), validation_dataset)\n",
    "np.save(os.path.join(save_dir, 'test_dataset.npy'), test_dataset)\n",
    "\n",
    "# Save the train, validation, and test labels\n",
    "np.save(os.path.join(save_dir, 'train_labels.npy'), train_labels)\n",
    "np.save(os.path.join(save_dir, 'validation_labels.npy'), validation_labels)\n",
    "np.save(os.path.join(save_dir, 'test_labels.npy'), test_labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
